# 1
> $\mathbb{R}^n$ är ett fullständigt rum

Relevanta definitioner:
> **Kontraktion**
> $g$ är en kontraktion $\iff$ $||g(x) - g(y)|| \leq L_g||x-y||$

> **Banachs Fixxpunktssats**
> Antag att $g : A \to A$ är en kontraktionsavbildning på den slutna mängden $A \in \mathbb{R}^n$, dvs
>
> $||g(x) - g(y)|| \leq L_g||x-y||\quad \forall x,y \in A,\quad L_g < 1$
>
> Då har g en unik fixpunkt i A, det vill säga
>
> $\exists! \bar{x} \in A : \bar{x} = g(\bar{x})$
>
> Fixpunkten fås som gränsvärde av fixpunktsiterationen $x_{j+1} = g(x_j)$ med $x_0 \in A$

> **Cauchy följd**
> $(x_k)^\infty_{k=0}$ är en cauchy-följd
> $\implies (x_k)^\infty_{k=0}$ konverget
> $\iff \exists! \bar{x} : \bar{x} \in X$ där $(\bar{x} = g(\bar{x}))$
> $x_{k+1} = g(x_k)$


För $\mathbb{R}^n$ blir således frågan ekvivalent med att bevisa att banach fixbunktsats stämmer för $\mathbb{R}^{n}$:
≈ För varje punkt $\bar{x}$ i $\mathbb{R}^n$ existerar en och endast en $\bar{x}$ där någon kontraktion $g \in \mathbb{R}^{n} \to \mathbb{R}^{n}$ har $\bar{x}$ som en fixpunkt, (att den mappar till sig själv)

## **Bevis:**

> RE:
> $x_{j+1} = g(x_j)$

Vi vill visa att:
$||g(x)-g(y)|| \leq L_g||x - y||$ 
$||g(x_j)-g(x_k)|| \leq L_g||x_j - x_k||$ (där $j>k$)
$||x_{j+1}-x_{k+1}|| \leq L_g||x_{j} - x_{k}||$ (där $j>k$)

Först, skriv genom triangelolikheten
$||x_{j+1}-x_{k+1}|| \leq L_g\sum_{i=k+1}^{j}||x_1-x_0||$

Om vi analyserar varje del av summan
$||x_{i+1}-x_{i+0}|| \leq L_g||x_{i+0} - x_{i-1}||$
$||x_{i+0}-x_{i-1}|| \leq L_g||x_{i-1} - x_{i-2}||$
$||x_{i-1}-x_{i-2}|| \leq L_g||x_{i-2} - x_{i-3}||$
$||x_{i-2}-x_{i-3}|| \leq L_g||x_{i-3} - x_{i-4}||$
$...$
$||x_{2}-x_{1}|| \leq L_g||x_{1} - x_{0}||$

Genom att komberina höger och vänster led får vi
$||x_{i+1}-x_{i}|| = L_g^i||x_{1} - x_{0}||$

Sätt tillbaka detta förenklade uttryck i summan
$||x_{j+1}-x_{k+1}|| \leq L_g\sum_{i=k+1}^{j}L_g^i||x_{1} - x_{0}||$

Summan av en geometrisk serie kan skrivas om
$||x_{j+1}-x_{k+1}|| \leq \frac{L_g(L_g^j-L_g^k)}{L_g-1}||x_0-x_1||$
$||x_{j+1}-x_{k+1}|| \leq \frac{(L_g^{j+1}-L_g^{k+1})}{L_g-1}||x_0-x_1||$
$||x_{j+1}-x_{k+1}|| \leq \frac{L_g^{k+1}(1-L_g^{j-k})}{1-L_g}||x_0-x_1||$

$||x_{j}-x_{k}|| \leq \frac{L_g^{k}(1-L_g^{j-k-1})}{1-L_g}||x_0-x_1||$

Då $L$ är mellan 0 och 1 går faktorn före $||x_1-x_0||$ tydligt mot oändligheten, vi kan altid hitta j/k som uppfyller att g är en kontraktion kriteriet.






# 2 Leibniz regel
> I flervariabelanalys definieras produktregeln som
$\large \frac{\partial h}{\partial x_i}
= \frac{\partial}{\partial x_i}(f \cdot g)
= \frac{\partial f}{\partial x_i} \cdot g + f \cdot \frac{\partial g}{\partial x_i}$
$\large \frac{\partial h}{\partial x_i}
= \frac{\partial}{\partial x_i}(f(x) \cdot g(x))
= \frac{\partial f}{\partial x_i} \cdot g(x) + f(x) \cdot \frac{\partial g}{\partial x_i}$
> Där $x \in \mathbb{R}^n$ och $f, g \in \mathbb{R}^n \to \mathbb{R}$

## **Bevis**

> RE: Definitionen av Derivatan
$\large \frac{\partial h}{\partial x_i} = \lim_{h \to 0} \frac{f(\bar{x}+h) - f(\bar{x})}{h}$
$\large \iff \lim_{h \to 0} \frac{f(\bar{x}+h) - f(\bar{x}) - f'(\bar{x})}{h} = 0$

$\large \frac{\partial h}{\partial x_i} = \frac{\partial}{\partial x_i}(f(x) \cdot g(x))$

$\large= \lim_{h \to 0} \frac{ f(x_1, ..., x_i + h, ..., x_n) \cdot g(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ..., x_n) \cdot g(x_1, ..., x_i, ..., x_n) }{h}$

$\large= \lim_{h \to 0} \frac{f(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ..., x_n)}{h} \cdot g(x_1, ..., x_i, ..., x_n)$
$\quad\large+ f(x_1, ..., x_i, ..., x_n) \cdot \frac{g(x_1, ..., x_i + h, ..., x_n) - g(x_1, ..., x_i, ..., x_n)}{h}$

Då vi förutsätter att derivatan exisiterar och inte är oändlig kan vi separera $\lim(a+b) \to (\lim a)+(\lim b)$
$\large= \left( \lim_{h \to 0} \frac{f(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ..., x_n)}{h} \cdot g(x_1, ..., x_i, ..., x_n) \right)$
$\quad\large+ \left( \lim_{h \to 0}f(x_1, ..., x_i, ..., x_n) \cdot \frac{g(x_1, ..., x_i + h, ..., x_n) - g(x_1, ..., x_i, ..., x_n)}{h} \right)$

Vi kan ta ut delarna operoende av h utanför limiten
$\large= \left( \lim_{h \to 0} \frac{f(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ..., x_n)}{h} \right) \cdot g(x_1, ..., x_i, ..., x_n)$
$\quad\large+ f(x_1, ..., x_i, ..., x_n) \cdot \left( \lim_{h \to 0} \frac{g(x_1, ..., x_i + h, ..., x_n) - g(x_1, ..., x_i, ..., x_n)}{h} \right)$

Dom $\lim$ som är kvar är identiska med de gränsvärdesdefinitionerna för egenstående partiella derivator.

$\Large
\frac{\partial h}{\partial x_i} = \frac{\partial f}{\partial x_i} \cdot g + f \cdot \frac{\partial g}{\partial x_i}
$





# 3 Taylors sats
> Antag $f : \mathbb{R}^n \to \mathbb{R}$ har kontinuerliga partiella derivator av ordning 3 i en omgivning av punkten $\bar{x}$. Då gäller Taylors formel för alla $x$ i denna omgivning, dvs:
>
> $f(x) = f(\bar{x}) + f'(\bar{x})h + \frac{1}{2}h^Tf''(\bar{x})h+E_2[f, \bar{x}](x)$ där $h=x-\bar{x}$
>
> Där resttermen uppfyller olikheten $|E[f,\bar{x}](x)| \le K||h||^3$ med $K = c_n max|f^{'''}_{ijk}(\zeta)|$ där maximum tas med avseende på $i, j, k$ och med avseende på alla $i$ den nämnda omgivningen.
>
> Polynomet
> $P_2[f,\bar{x}](x) = f(\bar{x}) + f'(\bar{x})(x-\bar{x}) + \frac{1}{2}(x-\bar{x})^Tf''(\bar{x})(x-\bar{x})$
> kallas Taylors polynom för $f$ i punkten $\bar{x}$.

> RE: Taylor för $x \in \mathbb{R}, F \in \mathbb{R} \to \mathbb{R}$
2: $F(x) = F(\bar{x}) + F'(\bar{x})(x-\bar{x}) + \frac{1}{2}F''(\bar{x})(x-\bar{x})^2 + E_2[F, \bar{x}](x)$
där $E_2[F, \bar{x}](x) = \frac{1}{6}F'''(s)(x-\bar{x})^3$ för något $s \in [x, \bar{x}]$
Taylor aproximeringen av funktionen $F$ taylor evaluaringen för punkten $\bar{t}$

Nu vill vi bevisa andra ordnings taylor för $f \in \mathbb{R}^n \to \mathbb{R}$

**Variabler**

$x \in \mathbb{R}^n$
$\bar{x} \in \mathbb{R}^n$
$h \in \mathbb{R}^n$
där $x = \bar{x} + h \iff h = x - \bar{x}$

$f \in \mathbb{R}^n \to \mathbb{R}$

## **Bevis**

### **1) Linarisera problemet**

låt $X(t) = f(\bar{x} + th)$ där $t\in\mathbb{R}, X\in\mathbb{R}\to\mathbb{R}$
Med denna får vi:
$X(0) = f(\bar{x})$ och
$X(1) = f(\bar{x}+h) = f(x)$
På så vis kan vi förhoppnings hitta en generell lösning runt $\bar{x}$ utan att behöva specificera riktiningen på h.

### **2) Taylor för $X$**

Om vi sen löser taylor av $X$ vid punkten 0 så har vi också löst taylor expansionen runt $\bar{x}$ för $f$
$X(t) = X(\bar{t}) + X'(\bar{t})(t-\bar{t}) + \frac{1}{2}X''(\bar{t})(t-\bar{t})^2 + E_2[X, \bar{t}](t)$
$X(t) = X(0) + X'(0)(t-0) + \frac{1}{2}X''(0)(t-0)^2 + E_2[X, 0](t)$
$X(t) = X(0) + X'(0)t + \frac{1}{2}X''(0)t^2 + E_2[X, 0](t)$

### **3) Lös X'**

$X'(t) = \frac{d}{dt}f(\bar{x} + th) = f'_1(\bar{x} + th)h_1 + f'_2(\bar{x} + th)h_2 + \cdots + f'_n(\bar{x} + th)h_n$
$X'(0) = f'_1(\bar{x})h_1 + f'_2(\bar{x})h_2 + \cdots + f'_n(\bar{x})h_n$
$= \begin{bmatrix}
f'_1(\bar{x}), f'_2(\bar{x}), \cdots , f'_n(\bar{x})
\end{bmatrix} {\tiny\begin{bmatrix}
h_1 \\
h_2 \\
\vdots \\
h_n
\end{bmatrix}}$
Om vi definierar $f'(\bar{x})$ som radvektorn av $f'$s får vi:
$X'(0) = f'(\bar{x})h$

### **4) Lös X''**

$X''(t) = \frac{d}{dt}^2X(t) = \frac{d}{dt}X'(t)$
$= \frac{d}{dt}(f'_1(\bar{x} + th)h_1 + f'_2(\bar{x} + th)h_2 + \cdots + f'_n(\bar{x} + th)h_n)$
Applicera kedjereglen och summera
$= f''_{11}(\bar{x} + th)h_1h_1 + f''_{12}(\bar{x} + th)h_1h_2 + \cdots + f''_{1n}(\bar{x} + th)h_1h_n$
$+ f''_{21}(\bar{x} + th)h_2h_1 + f''_{22}(\bar{x} + th)h_2h_2 + \cdots + f''_{2n}(\bar{x} + th)h_2h_n$
$+ \dots$
$+ f''_{n1}(\bar{x} + th)h_nh_1 + f''_{n2}(\bar{x} + th)h_nh_2 + \cdots + f''_{nn}(\bar{x} + th)h_nh_n$

$X''(0)$
$= f''_{11}(\bar{x})h_1h_1 + f''_{12}(\bar{x})h_1h_2 + \cdots + f''_{1n}(\bar{x})h_1h_n$
$+ f''_{21}(\bar{x})h_2h_1 + f''_{22}(\bar{x})h_2h_2 + \cdots + f''_{2n}(\bar{x})h_2h_n$
$+ \dots$
$+ f''_{n1}(\bar{x})h_nh_1 + f''_{n2}(\bar{x})h_nh_2 + \cdots + f''_{nn}(\bar{x})h_nh_n$

Detta går att förenkla till
$=\begin{bmatrix}
h_1, h_2, \cdots, h_n
\end{bmatrix}
\cdot
\begin{bmatrix}
f''_{11}(\bar{x}) & f''_{12}(\bar{x}) & \cdots & f''_{1n}(\bar{x}) \\
f''_{21}(\bar{x}) & f''_{22}(\bar{x}) & \cdots & f''_{2n}(\bar{x}) \\
\vdots & \vdots & \ddots & \vdots \\
f''_{n1}(\bar{x}) & f''_{n2}(\bar{x}) & \cdots & f''_{nn}(\bar{x}) \\
\end{bmatrix}
\cdot
\begin{bmatrix}
h_1 \\ h_2 \\ \vdots \\ h_n
\end{bmatrix}$
Om vi definierar $f''(\bar{x})$ som matrisen av $f''$s får vi:
$X''(0) = h^Tf''(\bar{x})h$

### **5) Lös ut f(x)**

> RE:
$X(t) = X(0) + X'(0)t + \frac{1}{2}X''(0)t^2 + E_2[X, 0](t)$
$X(t) = f(\bar{x} + th)$
$X(0) = f(\bar{x})$
$X(1) = f(\bar{x}+h)$

$f(x) = f(\bar{x}+h) = X(1)$
$\begin{array}{llllllllll}
   =& X(0)       &+& X'(0)1       &+& \frac{1}{2}X''(0)1^2        &+& E_2[X, 0](1) \\
   =& f(\bar{x}) &+& f'(\bar{x})h &+& \frac{1}{2}h^Tf''(\bar{x})h &+& E_2[X, 0](1) \\
   =& f(\bar{x}) &+& f'(\bar{x})h &+& \frac{1}{2}h^Tf''(\bar{x})h &+& E_2[f, \bar{x}](x)
\end{array}$

### **6) Motivera Resttermen**

Vi vill visa att: $|E[f,\bar{x}](x) \le K||h||^3$ med $K = c_n max|f^{'''}_{ijk}(\zeta)|$

<br>

> RE:
> $E_2[X, \bar{t}](t) = \frac{1}{6}X'''(s)(t-\bar{t})^3 \quad$ där $s \in [t, \bar{t}]$

$E_2[X, 0](1) = \frac{1}{6}X'''(s)(1-0)^3 = \frac{1}{6}X'''(s) \quad$ där $s \in [0, 1]$

$X'''(t) = \frac{d}{dt}{\Big(}$
$\quad f''_{11}(\bar{x} + th)h_1h_1 + f''_{12}(\bar{x} + th)h_1h_2 + \cdots + f''_{1n}(\bar{x} + th)h_1h_n$
$\quad + f''_{21}(\bar{x} + th)h_2h_1 + f''_{22}(\bar{x} + th)h_2h_2 + \cdots + f''_{2n}(\bar{x} + th)h_2h_n$
$\quad + \dots$
$\quad + f''_{n1}(\bar{x} + th)h_nh_1 + f''_{n2}(\bar{x} + th)h_nh_2 + \cdots + f''_{nn}(\bar{x} + th)h_nh_n$
${\Big)}$
Detta uttryck skulle bli extremt långt så jag introducerar en alias för partiel derivivering för den i:te dimensionen:
$k_i(\bar{x}+th) =$
$\quad   f'''_{11i}(\bar{x} + th)h_1h_1h_i + f'''_{12i}(\bar{x} + th)h_1h_2h_i + \cdots + f'''_{1ni}(\bar{x} + th)h_1h_nh_i$
$\quad + f'''_{21i}(\bar{x} + th)h_2h_1h_i + f'''_{22i}(\bar{x} + th)h_2h_2h_i + \cdots + f'''_{2ni}(\bar{x} + th)h_2h_nh_i$
$\quad + \dots$
$\quad + f'''_{n1i}(\bar{x} + th)h_nh_1h_i + f'''_{n2i}(\bar{x} + th)h_nh_2h_i + \cdots + f'''_{nni}(\bar{x} + th)h_nh_nh_i$

Med detta blir:
$X'''(t) = {\large\Sigma_{i=1}^n} k_i(\bar{x}+th)$

Om vi introducerar: $\zeta = \bar{x}+sh$ Får vi
$X(s) = f(\zeta)$
$X'''(s) = {\large\Sigma_{i=1}^n} k_i(\zeta)$

> RE: vi vill hitta storleken av
> $E_2[f, \bar{x}](x) = E_2[X, 0](1) = \frac{1}{6}X'''(s)$

Detta innebär att $X'''(s)$ är en lång skalärprodukt mellan $[f^{'''}_{ijk}(\zeta), \cdots]$ och $[h_xh_yh_z, \cdots]$
Det betyder att det största\* värdet möjliga värdet som kan ges av $X'''(s)$ är :
**Antalet summerade element** $\cdot$ **Största möjliga** $f^{'''}_{ijk}(\zeta)$ $\cdot$ **Största möjliga** $h_xh_yh_z$ `(PROP 1)`
(\*Med största i denna bemärkelse menar jag största absolutbelopp)

**Antalet summerade element** $= n^3$
**Största möjliga** $f^{'''}_{ijk}(\zeta) = max|f^{'''}_{ijk}(\zeta)|$
**Största möjliga** $h_xh_yh_z$ $= c \cdot ||h||^3$ där $c \le 1$

Om man flyttar runt $\frac{1}{6}$, $c$ och $n^3$ konstanterna så att dom räknas in i $K$ in är det enkelt att se att `(PROP 1)` är ekvivalent med det beviset efterfrågar:
> $|E_2[f,\bar{x}](x)| \le K||h||^3$ med $K = c_n max|f^{'''}_{ijk}(\zeta)|$

### **7) Resultat**

Vi har i steg 5 visat att:
$f(x) = f(\bar{x}+h) = X(1)$
$= f(\bar{x}) + f'(\bar{x})h + \frac{1}{2}h^Tf''(\bar{x})h + E_2[X, 0](1)$
$= f(\bar{x}) + f'(\bar{x})h + \frac{1}{2}h^Tf''(\bar{x})h + E_2[f,\bar{x}](x)$

där $f'(x) = \begin{bmatrix}
f'_1(\bar{x}), f'_1(\bar{x}), \cdots , f'_n(\bar{x})
\end{bmatrix}$
och $f''(x) =\begin{bmatrix}
f''_{11}(\bar{x}) & f''_{12}(\bar{x}) & \cdots & f''_{1n}(\bar{x}) \\
f''_{21}(\bar{x}) & f''_{22}(\bar{x}) & \cdots & f''_{2n}(\bar{x}) \\
\vdots & \vdots & \ddots & \vdots \\
f''_{n1}(\bar{x}) & f''_{n2}(\bar{x}) & \cdots & f''_{nn}(\bar{x}) \\
\end{bmatrix}$
(Hesse-matrisen)

Och sen i steg 6 har vi visat att:
$|E_2[f,\bar{x}](x)| \le K||h||^3$ med $K = c_n max|f^{'''}_{ijk}(\zeta)|$










# 4 Existens av skalär potential

> RE: Definition Konservativt
> Ett vektorfäkt $\bf{F}$ kallas konservativt i $D$ om det finns ett skalärt fänt $\phi$ sådant att:
>
> $\bf{F} = \nabla\phi \quad$ i $D$
>
> Fältet $\phi$ kallas då *potential* till $\bf{F}$

**Sats att bevisa**
> RE: Sats Tillräckligt vilkor för potential
> Antag att $\nabla \times {\bf{F}} = {\bf{0}}$ i ett enkelt sammangängande område $D$, Då är $\bf{F}$ konservativt, dvs det finns ett deriverbart fält $\phi$ sådant att ${\bf{F}} = \nabla\phi$ i $D$

## **Bevis**

### **1) $\phi$:s form**
Vi blörjar att använda Stoke theorem:
> RE:
> ${\bf S}$ är en yta
> $C$ är kantkurvan av ${\bf S}$ där $t$ går längs kurvan.
> $\iint_{\bf S}(\nabla \times {\bf F}) \cdot d{\bf S} = \int_C {\bf F} \cdot dt$

Sätter in $\nabla \times {\bf{F}} = {\bf{0}}$:
$\iint_{\bf S}({\bf 0}) \cdot d{\bf S} = \int_C {\bf F} \cdot dt$
$0 = \int_C {\bf F} \cdot dt$

Stokes stats visar att om vi ingegrarar runt kanten av en form i $\bf F$ får vi alltid skalären 0, oavsett vilken väg vi tar.
Vi delar upp sträckan i två delsträckor: $A$ och $B$. Om man integrerar längs båda efter varandrar får man 0.
$A$ från punkt $P$ till punkt $Q$
$B$ från punkt $Q$ till punkt $P$
![alt text](images/stokes-length-invariant-1.png)
Det interesanta här är hela varvet får kurvintegralen=0 oavsett formen av A (och/eller B).
⇒ Alla möjliga vägar fråm $P$ till $Q$ måste resultera i samma storlek av dess kurvintegral.
![alt text](images/stokes-length-invariant-2.png)

Således betyder det att om vi hittar **någon** $\phi$-kurva mellan två generiska startpunkter som visar $\bf{F} = \nabla\phi$ måste det påståendet också stämma för alla kurvor. (Så länge som $D$ är sammanhängande, vilket det är)

### **2) Definiera $\phi$ kurvan**

låt definiera $\phi$ som en godtycklig kurva mellan a och b i $\mathbb{R}^3$.
Skalären t bestämmer hur långt vi har gått från a till b längds med någon väg.
$\phi = \int_a^b {\bf F} \cdot dt$

Vi vill visa att vi kan hitta $a$, $b$ och vägen där imellan så att $\bf{F} = \nabla\phi$.

Vi vill visa:
$\nabla\phi = (\frac{\partial\phi}{\partial x}, \frac{\partial\phi}{\partial y}, \frac{\partial\phi}{\partial z})
 \stackrel{?}{=} ({\bf F}_1, {\bf F}_2, {\bf F}_3) = {\bf F}$

### **2.1) Hitta $\frac{\partial\phi}{\partial x}$**

$\large\frac{\partial\phi}{\partial x} = \lim_{h\to0}\frac{\phi(x+h,y,z)-\phi(x,y,z)}{h}$

Om vi väljer
$a = (x_0, y, z), b = (x_0+h, y, z)$

$\large\frac{\partial\phi}{\partial x} = \lim_{h\to0}\frac{
   \left({\large\int_{x_0,y,z}^{x_0+h,y,z}} {\bf F} \cdot dt\right)
   -\left({\large\int_{x_0,y,z}^{x_0,y,z}} {\bf F} \cdot dt\right)
}{h}$
$\large\frac{\partial\phi}{\partial x} = \lim_{h\to0}\frac{
   {\large\int_{x_0,y,z}^{x_0+h,y,z}} {\bf F} \cdot dt
}{h}$
Då vi endast rör oss längs kan vi förenkla x-axeln
$\large\frac{\partial\phi}{\partial x} = \lim_{h\to0}\frac{
   {\large\int_{x_0}^{x_0+h}} {\bf F_1}(x,y,z) \cdot dx
}{h}$
Detta uttryck representerar att ta derivatan av integralen, dessa tar ut varandra
> ⚠️ föutsatt att båda är väldefinierade, vilket vi antar att dom är.

Kvar blir:
$\frac{\partial\phi}{\partial x} = {\bf F_1}(x,y,z)$

### **2.2) Hitta $\frac{\partial\phi}{\partial y}$ & $\frac{\partial\phi}{\partial z}$**
om vi väljer
$a = (x, y_0, z), b = (x, y_0+h, z)$ repektive $a = (x, y, z_0), b = (x, y, z_0+h)$
kan vi göra samma sak som för $\frac{\partial\phi}{\partial x}$ för att visa att varje väg leder till ekvivalens med repektive ${\bf F}$

### **3) Slutsats**
Vi har visat att
$\nabla\phi = (\frac{\partial\phi}{\partial x}, \frac{\partial\phi}{\partial y}, \frac{\partial\phi}{\partial z})
= ({\bf F}_1, {\bf F}_2, {\bf F}_3) = {\bf F}$





# 5 Analysens fundamentalsats i flera variabler
> RE: i en variabel
> $\int^b_af(x)dx = F(b)-F(a) = \left[F(x)\right]^b_a$

**Sats att bevisa**
> $\int_C {\bf F} \cdot dr = \phi(r(b)) - \phi(r(a))$
> Givet att ${\bf F}$ är konservativt i $D$ ⇔ $({\bf F} = \nabla \phi)$

## **Bevis:**
$\int_C {\bf F} \cdot dr$
Är per definition *(Definition 5.3 (Tangentkurvintegral))*
$= \int_a^b {\bf F}(r(t)) \cdot r'(t)dt$
Applicera konservativt i $D$ kriteriet.
$= \int_a^b \nabla \phi(r(t)) \cdot r'(t)dt$

> RE: $\nabla$
> $\nabla\phi(r(t)) = \Big(
   \frac{\partial\phi}{\partial x}(r(t)),
   \frac{\partial\phi}{\partial y}(r(t)),
   \frac{\partial\phi}{\partial z}(r(t))
   \Big)$
> RE: Kedjeregeln
> $\frac{d}{dt} \phi(r(t))$
> $= \Big(
   \frac{\partial\phi}{\partial x}(r(t))\frac{\partial r}{\partial x}r(t),
   \frac{\partial\phi}{\partial y}(r(t))\frac{\partial r}{\partial y}r(t),
   \frac{\partial\phi}{\partial z}(r(t))\frac{\partial r}{\partial z}r(t)
  \Big)$
> $= \nabla\phi(r(t)) \cdot r'(t)$

Substituera kedjeregeln baklänges
$= \int_a^b \frac{d}{dt}\phi(r(t))dt$

Nu har vi integralen av en derivata, så vi kan använda envariabelens fundamentalsats
Mer precis har vi $\int^b_af'(t)dt = f(b)-f(a)$ där $f(t) = \phi(r(t))$,

Kvar får vi då
$= \phi(r(b)) - \phi(r(a))$

Vilket är precis vad vi ville bevisa.






# 6 Greens Sats
Greens sats fanns inte med i 2019 boken, denna definition (och bevisinspiration) är tagen från wikipedia + khan academy.

> RE: kurvintegral def
> $\int_C f\ ds = \int_a^b f(r(t))||r'(t)||dt$
> där $ds = ||r'(t)||dt$
> där $r(t)$ är en parametrisering av $C$
>
> För 2D: $||r'(t)|| = \sqrt{(\frac{dx}{dt})^2+(\frac{dy}{dt})^2}$

> RE: Ytinegral def
> $\iint_S f\ dS = \iint_D f(r(x,y)) ||\frac{\partial r}{\partial x}(x,y) \times \frac{\partial r}{\partial y}(x,y)||dxdy$
där $ds = ||\frac{\partial r}{\partial x}(x,y) \times \frac{\partial r}{\partial y}(x,y)||dxdy$
>
> Och ytan:
> $A = \iint_D||\frac{\partial r}{\partial x}(x,y) \times \frac{\partial r}{\partial y}(x,y)||dxdy$

**Sats att bevisa:**
> $\int_C (Pdx + Qdy) = \iint_D (\frac{\partial P}{\partial x} - \frac{\partial Q}{\partial y})dxdy$
> Där $\int_C$ är en kurvintegral **motsols** längs med kantent av ytan $D$.
> Där $P, Q \in \mathbb{R}^2 \to \mathbb{R}$

## **Bevis**
Beviset delas upp i olika former

### **1) Konvex**
Låt oss börja med att bevisa greens för en konvex region utan lodräta eller vågräta linjer D med kurvan C. Jag kallar dessa former **klass A**.
![alt text](images/green1.png)

$\int_C (Pdx + Qdy) = (\int_C Pdx) + (\int_C Qdy)$

### **1.1) Pdx**
Vi börjar med $\int_C Pdx$ och genom att kurvan i två delar som kan parametriseras av x (detta är varför kurvan måste vara konvex utan lodräta/vågräta linjer. Om så hade varit fallet hade fler än två delar behövts)
![alt text](images/green2.png)

$\int_C P(x,y)dx = {\int_C}_1 P(x,y)dx + {\int_C}_2 P(x,y)dx$

$= \Big( \int_a^b P(x, y_1(x))dx \Big) + \Big( \int_b^a P(x, y_2(x))dx \Big)$
$= \Big( \int_a^b P(x, y_1(x))dx \Big) - \Big( \int_a^b P(x, y_2(x))dx \Big)$
$= \int_a^b \Big(P(x, y_1(x)) - P(x, y_2(x))\Big)dx$
$= -\int_a^b \Big(P(x, y_2(x)) - P(x, y_1(x))\Big)dx$
$= -{\large\int_a^b} \Big[P(x,y)\Big]_{y=y_1(x)}^{y=y_2(x)}dx$
Förutsatt att derivatan existerar är kan vi göra en en bekant substitution:
$= -{\large\int_a^b} \Big( {\large\int_{y_1(x)}^{y_2(x)}} \frac{\partial P}{\partial y} dy \Big)dx$
$= -{\large\int_a^b} {\large\int_{y_1(x)}^{y_2(x)}} \frac{\partial P}{\partial y} dydx$

Detta är en ytintegral, och ytan som integreras över ${\large\int_a^b}{\large\int_{y_1(x)}^{y_2(x)}}$ är lika med $D$.
> ⚠️ Detta funkar då vi tog ut inverterade integrationsgränserna och tog ut minus före uttrycket. ${\large\int_a^b} {\large\int_{y_2(x)}^{y_1(x)}}$ är **inte** samma som $D$!

$= -\iint_D \frac{\partial P}{\partial y} dydx$

### **1.2) Qdy**
Nu gör vi något liknande för $\int_C Qdy$
> ⚠️ $a,b,c_1,c_2$ har helt annan innebörd här!

![alt text](images/green3.png)

$\int_C Q(x,y)dy = {\int_C}_1 Q(x,y)dy + {\int_C}_2 Q(x,y)dy$

$= \Big( \int_a^b Q(x_1(y), y)dy \Big) + \Big( \int_b^a Q(x_2(y), y)dy \Big)$
$= \Big( \int_a^b Q(x_1(y), y)dy \Big) - \Big( \int_a^b Q(x_2(y), y)dy \Big)$
$= \int_a^b \Big(  Q(x_1(y), y) - Q(x_2(y), y) \Big) dy$
$= {\large\int_a^b} \Big[Q(x,y)\Big]_{x=x_2(y)}^{x=x_1(y)}dy$
> ⚠️ Vi tar från $x_2 \to x_1$ inte $y_1 \to y_2$ som vi gjorde för $P$ så inget minus behövs!

Förutsatt att derivatan existerar är kan vi göra en en bekant substitution:
$= {\large\int_a^b} \Big( {\large\int_{x_2(x)}^{x_1(x)}} \frac{\partial Q}{\partial x} dx \Big)dy$
$= {\large\int_a^b} {\large\int_{x_2(x)}^{x_1(x)}} \frac{\partial Q}{\partial x} dxdy$

Detta är en ytintegral, och ytan som integreras över är igen lika med $D$

$= \iint_D \frac{\partial Q}{\partial x} dxdy$

### **1.3) Sätt ihop**
$\int_C (Pdx + Qdy) = (\int_C Pdx) + (\int_C Qdy)$
$= -\iint_D \frac{\partial P}{\partial y} dydx + \iint_D \frac{\partial Q}{\partial x} dxdy$
Då vi har tagit bort parametriseringen av $D$ är det ok att byta ordning på $dx$ och $dy$. (båda ordningar är lika med $dD$)
$= -\iint_D \frac{\partial P}{\partial y} dxdy + \iint_D \frac{\partial Q}{\partial x} dxdy$
Samma integrationsarea, så vi kan kombinera:
$= \iint_D \Big(-\frac{\partial P}{\partial y}+\frac{\partial Q}{\partial x} \Big) dxdy$
$= \iint_D \Big( \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \Big) dxdy$

Vilket är det vi ville bevisa!

### **2) Med raka linjer**
Låt oss nu bevisa greens för en liknande koveex form men nu har figuren en vågräta linje.
Igen: formen är D och den har motsolskurvan C. Jag kallar dessa former **klass B**.

![alt text](images/green_sq1.png)

Här har jag valt $C_k$ som den del av kurvan som är rak

> RE:
> $\int_C (Pdx + Qdy) = (\int_C Pdx) + (\int_C Qdy)$

### **2.1) Pdx**

![alt text](images/green_sq2.png)

$Pdx$ påverkas inte av det nya valet av form, parametriseringen går lika bra även denna gång.

Åter igen får vi:
$= -\iint_D \frac{\partial P}{\partial y} dydx$

### **2.2) Qdy**

![alt text](images/green_sq3.png)

$\int_C Qdx = \int_C Q(x,y)dy$
$= {\int_C}_1 Q(x,y)dy + {\int_C}_2 Q(x,y)dy + {\int_C}_3 Q(x,y)dy$

Då $y$ inte ändras längs $C_3$ är integationen längs denna sträcka lika med noll.
$\int_C Qdx = \int_C Q(x,y)dx = {\int_C}_1 Q(x,y)dy + {\int_C}_2 Q(x,y)dy$
$= {\int_C}_1 Q(x_a,y)dy + {\int_C}_2 P(x_b,y)dy$

$C_1$ och $C_2$ kan parametriseras lika dant som för formen utan en vågrät linje så uttrycket vi får
är igen identiskt med det för den konvexa icke lod/vågräta formen. Detta betyder att vi åter igen kommer landa i:
$= \iint_D \frac{\partial Q}{\partial x} dxdy$

### **2.3) Sätt ihop**
Då vi fick samma resultat av 2.2 och 2.3 som 1.2 och 2.3 får vi med identiskt resonemang:
$\int_C (Pdx + Qdy) = \iint_D \Big( \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \Big) dxdy$$

Med samma resonemang kan vi visa att green sats även stämmer för alla 90graders rotationer av klass B former.
![alt text](images/green_sq4.png)

### **3) Generallisering för alla ytor**
För en komplicerad figur kan man dela upp den i små delar och visa att green stämmer för alla delar individuellt.

Ta denna figur som exempel:
![alt text](images/green_ex1.png)
Och nu uppdelad i två klass B former.
![alt text](images/green_ex2.png)

Vi har redan bevisat att:
${\int_C}_{blå} (Pdx + Qdy) = {\iint_D}_{blå} (\frac{\partial P}{\partial x} - \frac{\partial Q}{\partial y})dxdy$
${\int_C}_{röd} (Pdx + Qdy) = {\iint_D}_{röd} (\frac{\partial P}{\partial x} - \frac{\partial Q}{\partial y})dxdy$

Och med hjälp av detta vill vi visa att:
${\int_C}_{svart} (Pdx + Qdy) = {\iint_D}_{svart} (\frac{\partial P}{\partial x} - \frac{\partial Q}{\partial y})dxdy$

⚠️ Vi förutsätter att $P$ och $Q$ (och dess yt/rand integraler) är definierat överallt längs och innuti formen!

### **3.1) Blå+Röd $\stackrel{?}{\Rarr}$ Svart**
Att summan av det blå och röda ytintegralen är det samma som den svarta är självklart, oavsett vilken funktion som integreras:
${\iint_D}_{svart} Fdxdy = {\iint_D}_{blå} Fdxdy + {\iint_D}_{röd} Fdxdy$

Är summan av randintegralerna av blå och röda är lika med den svara då?
${\int_C}_{svart} FdS \stackrel{?}{=} {\int_C}_{blå} FdS + {\int_C}_{röd} FdS$

Vi vet att:
${\int_C}_{svart} FdS = {\int_C}_{x} FdS + {\int_C}_{a} FdS$
${\int_C}_{blå} FdS = {\int_C}_{x} FdS + {\int_C}_{y} FdS$
${\int_C}_{röd} FdS = {\int_C}_{a} FdS + {\int_C}_{b} FdS$

Då $C_y$ och $C_b$ är exakt samma sträcka fast backlänges vet vi att:
${\int_C}_{b} FdS = -{\int_C}_{y} FdS$

Slutgiltigen sätter vi ihop det:
${\int_C}_{svart} FdS \stackrel{?}{=} {\int_C}_{blå} FdS + {\int_C}_{röd} FdS$
${\int_C}_{x} FdS + {\int_C}_{a} FdS \stackrel{?}{=} {\int_C}_{x} FdS + {\int_C}_{y} FdS + {\int_C}_{a} FdS + {\int_C}_{b} Fd$
${\int_C}_{x} FdS + {\int_C}_{a} FdS \stackrel{?}{=} {\int_C}_{x} FdS + {\int_C}_{y} FdS + {\int_C}_{a} FdS - {\int_C}_{y} Fd$
${\int_C}_{x} FdS + {\int_C}_{a} FdS \stackrel{?}{=} {\int_C}_{x} FdS + {\int_C}_{a} FdS$
${\int_C}_{x} FdS + {\int_C}_{a} FdS = {\int_C}_{x} FdS + {\int_C}_{a} FdS$ $\quad\square$

Vi har funnit att
${\int_C}_{svart} FdS = {\int_C}_{blå} FdS + {\int_C}_{röd} FdS$.

Altså:
${\iint_D}_{svart} Fdxdy = {\iint_D}_{blå} Fdxdy + {\iint_D}_{röd} Fdxdy$
${\int_C}_{svart} FdS = {\int_C}_{blå} FdS + {\int_C}_{röd} FdS$
Stämmer för alla $F$.

Således stämmer även:
${\int_C}_{svart} (Pdx + Qdy) = {\iint_D}_{svart} (\frac{\partial P}{\partial x} - \frac{\partial Q}{\partial y})dxdy$

Nu har vi visat att Greens även gäller för en form som man kan skära isär till två halvor där vardera halva är en Klass A/Klass B form.

Detta kan lätt bli ett induktionsbevis som visar att det gäller alla former, oavsett hur många gången man måste skära isär den. (Förutsatt att ett ändligt antal delningar leder till former som är klass A eller klass B)

### **3.2) Alla ytor?**

Det är enkelt att se att alla ytor kan konstrueras genom att skära isär dom till flera klass A/klass B figurer.

> Detta känns väldigt intuitivt men jag kommer inte på hur man formellt bevisar att så är fallet. Känns som det ligger utanför kursen?


### **4) Sammanfattning**

1) Vi har bevisat att greens sats gäller för alla former av klass A och klass B
2) Vi har motiverat att alla former kan skäras isär till klass A och klass B former
3) Vi har bevisat att green sats gäller för alla figurer som kan skäras isär till klass A och klass B former.
   - Vi har bevisat att för alla isärskurna figurer kommer summan av delformernas ytingegraler samma som den oskurna ytan.
   - Vi har bevisat att för alla isärskurna figurer kommer summan av delformernas randintegraler vara samma som den oskurna ytan.









# 7 Gauss sats

> RE: Slät yta
> En yta S kallas slät (eller glatt) om det finns ett kontinuerligt varierande enhetsnrmalvektorfält i varje punkt på ytan. Med andra ord: det ska finnas en parametrisering så att:
> $$\hat{\bf N} = \pm \frac{r'_u \times r_v'}{||r'_u \times r_v'||}$$
> existerar (⇔ $r'_u \times r_v' \ne \bf 0$).
> Man kan då välja en orientering av ytan. Ytan är en *styckvis slät orienterad yta* om den är hopskarvat av släta orienterade ytor så att orienteringarna från båda sidor av skavaen stämmer överäns
> Obs. $\hat{\bf N}$ behöver inte vara definiera på skarven mellan ytorna.

**Sats att bevisa**
> $\iiint_D \nabla \cdot {\bf F}dV = \iint_S \hat{\bf N} \cdot {\bf F}dS$
> Där D är en begränsad mängd i ${\Bbb R}^3$ vars rand är en styckvis slät orienterad yta S med ett utåtriktat enhetsnormalvektorfält $\hat{\bf N}$.
> Där $\bf F$ är ett vektorfält med kontinuerlig derivata i D

## **Bevis**

Detta bevis kommer göras relativt likt Greens sats, med bevis för en "snäll" volym som sen generaliseras. Den region vi väljer är konceptuellt lik klass B regionen från green sats
(också kallad typ 1 region?)

Vi antar att vi har en form som oavsett vilken enhetsriktning man kollar ifrån kan delas upp i två (eller tre där den tredje är parallell med en enhetsvektor) delar så att vardera region kan parametriseras (eller den tredje vars integral blir 0, likt $C_k$ i Greens)

![alt text](images/gauss_shape1.png)
Exempel av typ 1 region.

### **1.1) Bryt ner $\iiint_D$ & $\iint_S$**
> RE:
> $\nabla \cdot {\bf F} = (\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z})\cdot(F_1, F_2, F_3) = \frac{\partial F_1}{\partial x} + \frac{\partial F_3}{\partial y} + \frac{\partial F_2}{\partial z}$

$\large \iiint_D \nabla \cdot {\bf F}dV$
$\large =\iiint_D (\frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z})dV$
$\large =(\iiint_D \frac{\partial F_1}{\partial x}dV) + (\iiint_D \frac{\partial F_2}{\partial y}dV) + (\iiint_D \frac{\partial F_3}{\partial z}dV)$

$\large \iint_S \hat{\bf N} \cdot {\bf F}dS$
$=\large \iint_S (\hat{\bf N}_1 F_1 + \hat{\bf N}_2 F_2 + \hat{\bf N}_3 F_3)dS$
$=\large (\iint_S \hat{\bf N}_1 F_1 dS) + (\iint_S \hat{\bf N}_2 F_2 dS) + (\iint_S \hat{\bf N}_3 F_3 dS)$

Skriver framöver med x/y/z subscript istället för 1/2/3 då jag finner det tydligare att följa.
$=\large (\iint_S \hat{\bf N}_x F_x dS) + (\iint_S \hat{\bf N}_y F_y dS) + (\iint_S \hat{\bf N}_z F_z dS)$

### **1.2) Inspektera $\iint_S \hat{\bf N}_z F_z dS$**

Jag börjar här med $\iint_S \hat{\bf N}_z F_z dS$ då dess regioner är lättast att att rita ut.

Börja med att dela upp $S$ i parametriserbara regioner:
![alt text](images/gauss_shape_z.png)

$\iint_S \hat{\bf N}_z F_z dS = {\iint_S}_1 \hat{\bf N}_z F_z dS + {\iint_S}_2 \hat{\bf N}_z F_z dS +{\iint_S}_z \hat{\bf N}_z F_z dS$

Då riktningen av $S_3$ är ortagonal med z axeln, är $\hat{\bf N}_z$ alltid 0 här. Detta får följden att ${\iint_S}_3 \hat{\bf N}_z F_z dS$ också blir 0.

$\iint_S \hat{\bf N}_z F_z dS = {\iint_S}_1 \hat{\bf N}_z F_z dS + {\iint_S}_2 \hat{\bf N}_z F_z dS$

> ⚠️ om $S_3$ inte exsiterar (exempelvis om volymen är en boll) så gör det inget då ytintegralen av en tom region också är 0. Vi får fortfarande: $\iint_S \hat{\bf N}_z F_z dS = {\iint_S}_1 \hat{\bf N}_z F_z dS + {\iint_S}_2 \hat{\bf N}_z F_z dS$

### 1.2.1) parametrisering av S₁
Den konvexa formen av $S$ gör att vi kan parametrisera $S_1$ och $S_2$ så att:
$z_1 = f_1(x,y)$ för $S_1$
$z_2 = f_2(x,y)$ för $S_2$

${\iint_S}_1 \hat{\bf N}_z F_z(x, y, f_1(x,y)) dS$

> RE: 5.7 Ytintegral
> $\iint_S fdS = \iint_D f(r(u,v)) \Big\Vert \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v) \Big\Vert dudv$


> RE: 5.109
> $\hat{\bf N} = \LARGE\pm \frac{r'_u \times r_v}{\Vert r'_u \times r_v \Vert}$
>
> $\hat{\bf N} = \LARGE\pm \frac{\frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)}{\Vert \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v) \Vert}$
>
> Där valet av tecken innebär att vi väljer en orientering

Låt
$r(u,v) = (u, v, f_1(u,v))$

${\iint_S}_1 \hat{\bf N}_z F_z(x, y, f_1(x,y)) dS$
$= \large{\iint_D} F_z(r(u,v)) \left[\pm \frac{\frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)}{\Vert \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v) \Vert}\right]_z \Big\Vert \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v) \Big\Vert dudv$

$= \large{\iint_D} F_z(r(u,v)) \left[\pm \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)\right]_z dudv$


$= \large{\iint_D} F_z(r(u,v)) \left[\pm \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)\right]_z dudv$

> RE: kryssprodukt för z
> $c = a \times b$
> $c_z = a_x​b_y​−a_y​b_x​$

$r(u,v) = (u, v, f_1(u,v))$
$\frac{\partial r}{\partial u}(u,v) = (\frac{\partial u}{\partial u}, \frac{\partial v}{\partial u}, \frac{\partial f}{\partial u}) = (1,0, \frac{\partial f}{\partial u})$
$\frac{\partial r}{\partial v}(u,v) = (\frac{\partial u}{\partial v}, \frac{\partial v}{\partial v}, \frac{\partial f}{\partial v}) = (0,1, \frac{\partial f}{\partial v})$

$= \large{\iint_D} F_z(r(u,v)) \left(\pm \frac{\partial u}{\partial u}\frac{\partial v}{\partial v} - \frac{\partial v}{\partial u}\frac{\partial u}{\partial v} \right) dudv$
$= \large{\iint_D} F_z(r(u,v)) (\pm (1-0)) dudv$

väljer $u=x, v=y$ och att $D$ är projektionen av $S_1$ på $xy$ planet.

$= \large{\iint_D} F_z(x, y, f_1(x,y)) (\pm 1) dxdy$

Nu måste man vara försiktig när man väljer rätt $\pm$. $S_1$ är ovansidan av figuren, normalen bör därför peka i samma riktning som kryssprodukten mellan $\hat{\imath}$ (det vi satte valde som u) och $\hat{\jmath}$ (det vi satte valde som v). XYZ är högerhänt i bilden, således väljer vi den positiva varianten och med det samma riktining som $z$:

${\iint_S}_1 \hat{\bf N}_z F_z dS = \large{\iint_D} F_z(x, y, f_1(x,y)) dxdy$

### **1.2.2) Parametrisering av S**₂

Den enda skillnaden i konverteringen av $S_2$ gämfört med $S_1$ är vid valet av riktning av normalen.
Då $S_2$ är undersidan och pekar bort från positiva z riktningen. Vi får därför ett negativt uttryck.
${\iint_S}_2 \hat{\bf N}_z F_z dS = -\large{\iint_D} F_z(x, y, f_2(x,y)) dxdy$

### **1.2.3) Hela ytan**
Ytan $D$ vi får är samma för både $S_1$ och $S_2$

${\iint_S}_2 \hat{\bf N}_z F_z dS = \large{\iint_D} F_z(x, y, f_1(x,y)) dxdy-\large{\iint_D} F_z(x, y, f_2(x,y)) dxdy$

$= \large {\iint_D} \Big( F_z(x, y, f_1(x,y)) -F_z(x, y, f_2(x,y))\Big) dxdy$
Där $D$ är projektionen av $V$ (eller $S$) på $xy$ planet.

> ⚠️ Inte samma D som i trippelintegralen!

### 1.3) **Inspektera $\iiint_D \frac{\partial F_z}{\partial z}dV$**

$\iiint_D \frac{\partial F_z}{\partial z}dV$

På grund av formvalet vet vi att övre/undre ytan av $S$ går att parametriseras för $x,y$ kan vi uttrycka integrationen i två steg så att $z$ initialt är oberoende av att veta $x$ och $y$.

$=\iint_{D} \left( \int_{f_2(x,y)}^{f_1(x,y)} \frac{\partial F_z}{\partial z}dz \right) dydx$

> ⚠️ Inte samma D som i trippelintegralen!
> ⚠️ Vi ingererar från den lägre begränsningen $f_2$ (undersidan av figuren) till den högre $f_1$ (ovansidan av figuren)

Inuti parantesen kan vi använda fundmentalsatsen baklänges för att förenkla. (Efterssom vi tar integralen av derivatan)

$=\iint_{D} \left[ \frac{\partial F_z}{\partial z}dz \right]_{f_2(x,y)}^{f_1(x,y)} dydx$
$=\iint_{D} \left(F_z(x,y,f_1(x,y)) - F_z(x,y,f_2(x,y)) \right) dydx$

Detta är samma uttryck som för 1.2!

> ⚠️Dubbelintegralerna har samma $D$, det finns bara en yta det kan representera och det är den måste begränsas av samma $x,y$

### **1.4) Sammanfattning $F_z$**
$\large \iint_S \hat{\bf N}_z F_z dS = \iiint_D \frac{\partial F_z}{\partial z}dV$


### **2) Hur blir det för $F_x$ & $F_y$ ?**
För $F_x$ och $F_y$ går det stort sett kopiera samma resonemang som för $F_z$. Det jag ser värt att dubbelkolla är kryssprodukterna då dom kan bli annorluna beroende på axlar. Resonemang angångende trippelintegralen är oberoende av kryssprodukter.

### **2.1) Dubbelkolla $F_x$**
![alt text](images/gauss_shape_x.png)

> RE: kryssprodukt för x
> $c = a \times b$
> $c_x = a_y​b_z​−a_z​b_y$

$r(u,v) = (f_1(u,v), u, v)$
$\frac{\partial r}{\partial u}(u,v) = (\frac{\partial f}{\partial u}, \frac{\partial u}{\partial u}, \frac{\partial v}{\partial u}) = (\frac{\partial f}{\partial u}, 1, 0)$
$\frac{\partial r}{\partial v}(u,v) = (\frac{\partial f}{\partial v}, \frac{\partial u}{\partial v}, \frac{\partial v}{\partial v}) = (\frac{\partial f}{\partial v}, 0, 1)$

$= \large{\iint_D} F_x(r(u,v)) \left[\pm \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)\right]_x dudv$

$= \large{\iint_D} F_x(r(u,v)) \left(\pm \frac{\partial u}{\partial u}\frac{\partial v}{\partial v} - \frac{\partial v}{\partial u}\frac{\partial u}{\partial v} \right) dudv$
$= \large{\iint_D} F_x(r(u,v)) (\pm (1-0)) dudv$

Yes, kryssprodukten stämmer, samma resultat som för $F_z$

### **2.2) Dubbelkolla $F_y$**
![alt text](images/gauss_shape_y.png)

> RE: kryssprodukt för y
> $c = a \times b$
> $c_y = a_zb_x​−a_x​b_z$

$r(u,v) = (u, f_1(u,v), v)$
$\frac{\partial r}{\partial u}(u,v) = (\frac{\partial u}{\partial u}, \frac{\partial f}{\partial u}, \frac{\partial v}{\partial u}) = (1, \frac{\partial f}{\partial u}, 0)$
$\frac{\partial r}{\partial v}(u,v) = (\frac{\partial u}{\partial v}, \frac{\partial f}{\partial v}, \frac{\partial v}{\partial v}) = (0, \frac{\partial f}{\partial v}, 1)$

$= \large{\iint_D} F_y(r(u,v)) \left[\pm \frac{\partial r}{\partial u}(u,v) \times \frac{\partial r}{\partial v}(u,v)\right]_y dudv$

$= \large{\iint_D} F_y(r(u,v)) \left(\pm \frac{\partial v}{\partial u}\frac{\partial u}{\partial v} - \frac{\partial u}{\partial u}\frac{\partial v}{\partial v} \right) dudv$
$= \large{\iint_D} F_y(r(u,v)) (\pm (0-1)) dudv$

Istället för $1$ blev kryssprodukten $-1$ men då vi väljer $\pm$ ser vi till att det pekar åt rätt håll.

⇒ Samma resultat som för $F_z$

> Footnote
> om man väljer $r(u,v) = (v, f_1(u,v), u)$ får man återigen $1$. Valet av vilken som mappas till $u$ eller $v$ ska inte spela roll.

### 3 Sammanfattning för "snälla" former:

$\large \iint_S \hat{\bf N} \cdot {\bf F}dS$
$=\large (\iint_S \hat{\bf N}_x F_x dS) + (\iint_S \hat{\bf N}_y F_y dS) + (\iint_S \hat{\bf N}_z F_z dS)$

$\large \iiint_D \nabla \cdot {\bf F}dV$
$\large =(\iiint_D \frac{\partial F_x}{\partial x}dV) + (\iiint_D \frac{\partial F_x}{\partial y}dV) + (\iiint_D \frac{\partial F_x}{\partial z}dV)$

$\large \iint_S \hat{\bf N}_x F_x dS = \iiint_D \frac{\partial F_x}{\partial x}dV$
$\large \iint_S \hat{\bf N}_y F_y dS = \iiint_D \frac{\partial F_y}{\partial y}dV$
$\large \iint_S \hat{\bf N}_z F_z dS = \iiint_D \frac{\partial F_z}{\partial z}dV$

$\implies \large \iint_S \hat{\bf N} \cdot {\bf F}dS = \large \iiint_D \nabla \cdot {\bf F}dV$
Vilket är det vi ville visa:

### **4) Generalisering för andra former**
Vi kommer nu se vad som händer när man delar på figuren för att se om:
Figur går att dela i "snälla" delar $\stackrel{?}{\implies}$ Gauss divergenssats håller för hela formen.


Låt oss ge ett vissuelt exempel:
![alt text](images/gauss_wiki1.png)
(Bilden är tagen från wikipedia)

> ⚠️ för att passa med bilden används $V$ istället för $D$ för trippelintegralen.

> Observationer:
> $V = V_1 \cup V_2$
> $S = S_1 \cup S_2$
> ${\iiint_V} \nabla \cdot {\bf F}dV= {\iiint_V}_1 \nabla \cdot {\bf F}dV + {\iiint_V}_2 \nabla \cdot {\bf F}dV$

> Observationer ang. ytingegral (flux).
> $\phi_S = \iint_S \hat{\bf N} \cdot {\bf F}dS = \phi_1 + \phi_2$
>
> $\phi_{1}  = {\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS$
> $\phi_{31} = {\iint_S}_3 \hat{\bf N} \cdot {\bf F}dS$
> $\phi_{2}  = {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS$
> $\phi_{32} = {\iint_S}_3 (-\hat{\bf N}) \cdot {\bf F}dS$
> (Normalen av $S_3$ är vara motsatt riktning från $V_1$, annars blir $V_2$ inte orienterad konsekvent)

Det som flöder ut från $V_1$ genom $S_3$ måste natruligtvis vara lika stort som det som flöder in till $V_2$ genom $S_3$
$\phi_{31} =  -\phi_{32}$
⇒
$\iint_S \hat{\bf N} \cdot {\bf F}dS = \phi_1 + \phi_{31} + \phi_2 + \phi_{32}$
$\iint_S \hat{\bf N} \cdot {\bf F}dS =$ Flödet genom väggarna av $V_1$ + flödet genom väggarna av $V_2$

### **4.1) Relatera tillbaks till divergenssatsen**
Säg att vi har delat en icke-snäll form till två snälla former, kan vi då visa att divergenssatsen funkar?

$\iint_S \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=} \large \iiint_V \nabla \cdot {\bf F}dV$

$\iint_S \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=} {\iiint_V}_1 \nabla \cdot {\bf F}dV + {\iiint_V}_2 \nabla \cdot {\bf F}dV$

Applicera divergensatsen för $V_1$ och $V_2$

$\iint_S \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=}
\left( {\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_3 \hat{\bf N} \cdot {\bf F}dS \right) +
\left( {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_3 (-\hat{\bf N}) \cdot {\bf F}dS \right)$
$\iint_S \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=}
\left( {\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_3 \hat{\bf N} \cdot {\bf F}dS \right) +
\left( {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS - {\iint_S}_3 \hat{\bf N} \cdot {\bf F}dS \right)$
$\iint_S \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=}
{\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS$

Dela upp $\phi_S$ i $\phi_1$ och $\phi_2$

${\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS \stackrel{?}{=}
{\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS$
${\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS =
{\iint_S}_1 \hat{\bf N} \cdot {\bf F}dS + {\iint_S}_2 \hat{\bf N} \cdot {\bf F}dS$ $\quad\square$

⇒ om divergenssaten stämmer för båda halvorna stämmer den även för hela formen.

Nu har vi visat att Gauss divergenssats även gäller för alla form som kan skära isär till två halvor där vardera halva är en "snäll" form.

Detta kan lätt bli ett induktionsbevis som visar att det gäller alla former, oavsett hur många gången man måste skära isär den. (Förutsatt att ett ändligt antal delningar leder till att alla delar blir "snälla")

### **4.2) Kan allt delas?**
Igen, likt Greens kan jag inte komma på ett formellt bevis att alla former faktiskt kan skäras isär till "snälla" bitar. Det känns intutivt trots att det inte är lika glasklart att som i 2D.

Tar detta som något utanför kursen. Skulle vara kul att veta om det existerar något enkelt bevis.

### **5) Sammanfattning**

1) Vi har bevisat att Gauss divergenssats gäller för alla "snälla" former
2) Vi har motiverat att alla former kan skäras isär till "snälla" former
3) Vi har bevisat att Gauss divergenssats gäller för alla figurer som kan skäras isär till "snälla" former.

En snäll form är en konvex form som, för varje $x,y,z$ riktning, kan uppdelas i två (eller tre där den tredje är parallell med den valda riktningsvektorn) så att varje halva kan parametriseras utifrån de två resterande $x,y,z$.









# 8 Basfunktioneras derivator
Sats ang. basfunktionerna fanns inte med i 2019 boken

> RE: 1D
> En kontinuerlig styckvis linjär funktion $U(x)$ är entydligt bestämd  av sida nodvärden $U_i = U(x_i)$. För att beskriva U(x) inför vi *basfunktionerna* $\phi_i(x)$, en för varje nod $x_i$. Funktionera $\phi_i(x)$ bestäms av att det är kontinuerliga, styckvis linjära, samt att
> $\phi_i(x_j) = \begin{cases}
   1 &\text{om } i = j \\
   0 &\text{om } i \ne j
\end{cases}$
> Funktionen $U(x)$ kan nu bkrivas som en linjär kombination av basfunktionerna:
> $U(x) = \displaystyle\sum_{i=1}^N U_i\phi_i(x)$, med koeffcienterna $U_i = U(x_i)$
>
>> Obs att
>> $U(x_j) = \displaystyle\sum_{i=1}^N U_i\phi_i(x_j) = U_j$
>> efterssom $\phi_i(x_j) = \begin{cases}
   1 &\text{om } i = j \\
   0 &\text{om } i \ne j
\end{cases}$ innebär det att endast en term (där $i=j$) blir kvar i summan
>
> Vi har nu en formel som uttrycker $U(x)$ med hjälp av nodväderna. Vi ska nu bestämma de okända nodvärderna $U_i$ så att $U(x)$ blir en approximatic lösning till randvärdesproblemet (3.5). Vi andäner den svaga formulering (3.14).
>
>> RE: 3.5 (Definition 3.1 randvärdesproblem)
>> Finn $u$ sådam att:
>> $-D(aDu) = f\quad$ för $x \in I = (0,L)$
>> $aD_Nu+k(u-u_A) = g\quad$ för $x = 0,L$
>
>> RE: 3.14 (Definition 3.2 Sag formulering)
>> Finn en funktion $u$ sådan att ekvationen
>> $\int_0^L aDuDvdx + k_0u(0)v(0) + k_Lu(L)v(L)$
>> $= \int_0^L fvdx + (k_0u_0+g_0)v(0) + (k_Lu_L + g_L)v(L)$
>
> För att det inte ska bli så mycket att skriva genomför vi detta i fallet då $k_0 = k_l = 0, g_0 = g_l = 0$:
> $\int_0^L aDuDvdx = \int_0^L fvdx\quad$ (för alla $v$)
> Istället för $u(x)$ sätter vi in ansatsen $U(x) = \sum_{i=1}^N U_i\phi_i(x)$ och väljer testfunktionerna $v = \phi_j$
> Vi får
> $\displaystyle\sum_{i=1}^N U_i \int_0^L aD\phi_iD\phi_kdx = \int_0^L f\phi_jdx$
>
> Med betäckningarna
> $a_{ij} = a_{ji} = \int_0^L aD\phi_iD\phi_kdx,\quad b_j=\int_0^L f\phi_jdx$
> blir detta
> $\displaystyle\sum_{i=1}^N a_{ji} U_i = b_j,\quad j=1,\cdots,N$
> dvs på matrisform:
> $AU = b$
>
> Detta är ett linjärt ekvationssystem av N ekvationer för N obekanta. Matrisen
> $A = \left\{ a_{ij} \right\}^N_{i,j=1} = \left\{ \int_0^L aD\phi_iD\phi_kdx \right\}^N_{i,j=1}$
> Kallas *styvhetsmatris* (*stiffnes matrix*). Jämför med stångens ekvations där a = EA är materialsets styvhet. Styvhetsmatrisen är symmetrisk, ($a_{ji} = a_{ij}$), och *tridiagonal*
> $$A = \def\arraystretch{1.5}\begin{bmatrix}
   \ast   & \ast   & 0      & \cdots & 0 \\
   \ast   & \ast   & \ast   & \ddots & \vdots \\
   0      & \ast   & \ast   & \ast   & 0 \\
   \vdots & \ddots & \ast   & \ast   & * \\
   0      & \cdots & 0      & \ast   & *
\end{bmatrix}$$
>
>> RE: motsvarande för 2d
>> $\displaystyle\sum_{i=1}^N U_i
\underbrace{\iint_D a \nabla\phi_i \cdot \nabla\phi_j dA}_{\large a_{ji}}
= \underbrace{\iint_D f\phi_jdA}_{\large b_{j}}
,\quad j=1,\cdots,N$
>> Blir igen:
>> $AU = b$

> RE: basfunktionerna
> $\left\{ \phi_i \right\}^N_{i=1}$

**Sats att bevisa?**
> Basfunktionernas derivator...?

**Bevis**
Då basfunktionerna är styckvis linjära är deras derivata styckvisa konstanter?
Fattar inte frågan utan att veta vad satsen är.